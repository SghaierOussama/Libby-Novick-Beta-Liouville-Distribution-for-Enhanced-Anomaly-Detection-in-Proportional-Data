{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUGw6b2-LJre"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
        "from keras.datasets import mnist, fashion_mnist, cifar100, cifar10\n",
        "from keras.backend import cast_to_floatx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0MHJ4pVLoec"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Add, Activation, Dropout, Flatten, Dense\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D\n",
        "#from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.normalization.batch_normalization_v1 import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "import random\n",
        "\n",
        "random.seed(0)\n",
        "tf.random.set_seed(0)\n",
        "weight_decay = 0.0000005"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi93ip8mLuBm"
      },
      "outputs": [],
      "source": [
        "import abc\n",
        "import itertools\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import apply_affine_transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqege8c9MAiX"
      },
      "source": [
        "## **Load and Preprocessing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgXT02KNLzm6"
      },
      "outputs": [],
      "source": [
        "def resize_and_crop_image(input_file, output_side_length, greyscale=False):\n",
        "    img = cv2.imread(input_file)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB if not greyscale else cv2.COLOR_BGR2GRAY)\n",
        "    height, width = img.shape[:2]\n",
        "    new_height = output_side_length\n",
        "    new_width = output_side_length\n",
        "    if height > width:\n",
        "        new_height = int(output_side_length * height / width)\n",
        "    else:\n",
        "        new_width = int(output_side_length * width / height)\n",
        "    resized_img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
        "    height_offset = (new_height - output_side_length) // 2\n",
        "    width_offset = (new_width - output_side_length) // 2\n",
        "    cropped_img = resized_img[height_offset:height_offset + output_side_length,\n",
        "                              width_offset:width_offset + output_side_length]\n",
        "    assert cropped_img.shape[:2] == (output_side_length, output_side_length)\n",
        "    return cropped_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoWPCnk3MOa1"
      },
      "outputs": [],
      "source": [
        "def normalize_minus1_1(data):\n",
        "    return 2*(data/255.) - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHgParnmMRw7"
      },
      "outputs": [],
      "source": [
        "def get_channels_axis():\n",
        "  import keras\n",
        "  idf = keras.backend.image_data_format()\n",
        "  if idf == 'channels_first':\n",
        "      return 1\n",
        "  assert idf == 'channels_last'\n",
        "  return 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygY-qVKqMVB3"
      },
      "outputs": [],
      "source": [
        "def load_fashion_mnist():\n",
        "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "    X_train = normalize_minus1_1(cast_to_floatx(np.pad(X_train, ((0, 0), (2, 2), (2, 2)), 'constant')))\n",
        "    X_train = np.expand_dims(X_train, axis=get_channels_axis())\n",
        "    X_test = normalize_minus1_1(cast_to_floatx(np.pad(X_test, ((0, 0), (2, 2), (2, 2)), 'constant')))\n",
        "    X_test = np.expand_dims(X_test, axis=get_channels_axis())\n",
        "    return (X_train, y_train), (X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mnist():\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    X_train = normalize_minus1_1(cast_to_floatx(np.pad(X_train, ((0, 0), (2, 2), (2, 2)), 'constant')))\n",
        "    X_train = np.expand_dims(X_train, axis=get_channels_axis())\n",
        "    X_test = normalize_minus1_1(cast_to_floatx(np.pad(X_test, ((0, 0), (2, 2), (2, 2)), 'constant')))\n",
        "    X_test = np.expand_dims(X_test, axis=get_channels_axis())\n",
        "    return (X_train, y_train), (X_test, y_test)"
      ],
      "metadata": {
        "id": "vSn04aB8mzvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cifar10():\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "    X_train = normalize_minus1_1(cast_to_floatx(X_train))\n",
        "    X_test = normalize_minus1_1(cast_to_floatx(X_test))\n",
        "    return (X_train, y_train), (X_test, y_test)"
      ],
      "metadata": {
        "id": "mIG39T3TPeGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZL18EvHMl8b"
      },
      "source": [
        "## **Saving the results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7NtHmLmMfoN"
      },
      "outputs": [],
      "source": [
        "def roc_pr_curve_data(scores, labels):\n",
        "    scores = scores.flatten()\n",
        "    labels = labels.flatten()\n",
        "\n",
        "    scores_pos = scores[labels == 1]\n",
        "    scores_neg = scores[labels != 1]\n",
        "\n",
        "    truth = np.concatenate((np.zeros_like(scores_neg), np.ones_like(scores_pos)))\n",
        "    preds = np.concatenate((scores_neg, scores_pos))\n",
        "    fpr, tpr, roc_thresholds = roc_curve(truth, preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc\",roc_auc)\n",
        "\n",
        "    # pr curve where \"normal\" is the positive class\n",
        "    precision_norm, recall_norm, pr_thresholds_norm = precision_recall_curve(truth, preds)\n",
        "    pr_auc_norm = auc(recall_norm, precision_norm)\n",
        "    print(\"pr_auc_norm where normal is the positive class\",pr_auc_norm)\n",
        "\n",
        "    # pr curve where \"anomaly\" is the positive class\n",
        "    precision_anom, recall_anom, pr_thresholds_anom = precision_recall_curve(truth, -preds, pos_label=0)\n",
        "    pr_auc_anom = auc(recall_anom, precision_anom)\n",
        "    print(\"pr_auc_norm where anomaly is the positive class\",pr_auc_anom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEEuib19NZ26"
      },
      "outputs": [],
      "source": [
        "def get_class_name_from_index(index, dataset_name):\n",
        "    ind_to_name = {\n",
        "        'fashion-mnist': ('t-shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag',\n",
        "                          'ankle-boot'),\n",
        "    }\n",
        "\n",
        "    return ind_to_name[dataset_name][index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA18t2j0Mq9Z"
      },
      "source": [
        "## **Transformations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOM4GuZqMqDS"
      },
      "outputs": [],
      "source": [
        "class AffineTransformation(object):\n",
        "    def __init__(self, flip, tx, ty, k_90_rotate):\n",
        "        self.flip = flip\n",
        "        self.tx = tx\n",
        "        self.ty = ty\n",
        "        self.k_90_rotate = k_90_rotate\n",
        "\n",
        "    def __call__(self, x):\n",
        "        res_x = x\n",
        "        if self.flip:\n",
        "            res_x = np.fliplr(res_x)\n",
        "        if self.tx != 0 or self.ty != 0:\n",
        "            res_x = apply_affine_transform(res_x, tx=self.tx, ty=self.ty,row_axis=0,\n",
        "    col_axis=1, channel_axis=2, fill_mode='reflect')\n",
        "        if self.k_90_rotate != 0:\n",
        "            res_x = np.rot90(res_x, self.k_90_rotate)\n",
        "\n",
        "        return res_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QRX0FkLMybA"
      },
      "outputs": [],
      "source": [
        "class AbstractTransformer(abc.ABC):\n",
        "    def __init__(self):\n",
        "        self._transformation_list = None\n",
        "        self._create_transformation_list()\n",
        "\n",
        "    @property\n",
        "    def n_transforms(self):\n",
        "        return len(self._transformation_list)\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def _create_transformation_list(self):\n",
        "        return\n",
        "\n",
        "    def transform_batch(self, x_batch, t_inds):\n",
        "        assert len(x_batch) == len(t_inds)\n",
        "\n",
        "        transformed_batch = x_batch.copy()\n",
        "        for i, t_ind in enumerate(t_inds):\n",
        "            transformed_batch[i] = self._transformation_list[t_ind](transformed_batch[i])\n",
        "        return transformed_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "totupBB0M1PE"
      },
      "outputs": [],
      "source": [
        "class Transformer(AbstractTransformer):\n",
        "    def __init__(self, translation_x=8, translation_y=8):\n",
        "        self.max_tx = translation_x\n",
        "        self.max_ty = translation_y\n",
        "        super().__init__()\n",
        "\n",
        "    def _create_transformation_list(self):\n",
        "        transformation_list = []\n",
        "        for is_flip, tx, ty, k_rotate in itertools.product((False, True),\n",
        "                                                           (0, -self.max_tx, self.max_tx),\n",
        "                                                           (0, -self.max_ty, self.max_ty),\n",
        "                                                           range(2)):\n",
        "            transformation = AffineTransformation(is_flip, tx, ty, k_rotate)\n",
        "            transformation_list.append(transformation)\n",
        "\n",
        "        self._transformation_list = transformation_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFHtMXniM4nl"
      },
      "outputs": [],
      "source": [
        "class SimpleTransformer(AbstractTransformer):\n",
        "    def _create_transformation_list(self):\n",
        "        transformation_list = []\n",
        "        for is_flip, k_rotate in itertools.product((False, True),\n",
        "                                                    range(2)):\n",
        "            transformation = AffineTransformation(is_flip, 0, 0, k_rotate)\n",
        "            transformation_list.append(transformation)\n",
        "\n",
        "        self._transformation_list = transformation_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO5wCGPjM9zT"
      },
      "source": [
        "## **The Model: Wide Residual Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QW0y3hiLM8bC"
      },
      "outputs": [],
      "source": [
        "def initial_conv(input):\n",
        "    x = Convolution2D(16, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(input)\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def expand_conv(init, base, k, strides=(1, 1)):\n",
        "    x = Convolution2D(base * k, (3, 3), padding='same', strides=strides, kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(base * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    skip = Convolution2D(base * k, (1, 1), padding='same', strides=strides, kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    m = Add()([x, skip])\n",
        "\n",
        "    return m\n",
        "\n",
        "\n",
        "def conv1_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv2_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"th\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv3_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"th\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def create_wide_residual_network(input_dim, nb_classes=100, N=2, k=1, dropout=0.0, verbose=1):\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    ip = Input(shape=input_dim)\n",
        "\n",
        "    x = initial_conv(ip)\n",
        "    nb_conv = 4\n",
        "\n",
        "    x = expand_conv(x, 16, k)\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv1_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 32, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv2_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 64, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv3_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = AveragePooling2D((8, 8))(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    x = Dense(nb_classes, kernel_regularizer=l2(weight_decay), activation='softmax')(x)\n",
        "\n",
        "    model = Model(ip, x)\n",
        "\n",
        "    if verbose: print(\"Wide Residual Network-%d-%d created.\" % (nb_conv, k))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df2kvANONx2t"
      },
      "source": [
        "## **Experiments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpVXqZTTN-Hi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "from collections import defaultdict\n",
        "from glob import glob\n",
        "from datetime import datetime\n",
        "from multiprocessing import Manager, freeze_support, Process\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "from scipy.special import psi, polygamma, gamma\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from joblib import Parallel, delayed\n",
        "from keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input\n",
        "from keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIOm47AHOIOj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from glob import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
        "from keras.datasets import mnist, fashion_mnist, cifar100, cifar10\n",
        "from keras.backend import cast_to_floatx\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSJ4PYiINJwB"
      },
      "outputs": [],
      "source": [
        "def _transformations_experiment(dataset_load_fn, dataset_name, single_class_ind, gpu_q):\n",
        "    gpu_to_use = gpu_q.get()\n",
        "    print('training class:',single_class_ind)\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_to_use\n",
        "\n",
        "    (x_train, y_train), (x_test, y_test) = dataset_load_fn()\n",
        "    x_train = x_train[:TRAIN_SIZE]\n",
        "    y_train =  y_train[:TRAIN_SIZE]\n",
        "    x_test =  x_test[:VAL_SIZE]\n",
        "    y_test =  y_test[:VAL_SIZE]\n",
        "\n",
        "    transformer = Transformer(8, 8)\n",
        "    n, k = (2, 8)\n",
        "    mdl = create_wide_residual_network(x_train.shape[1:], transformer.n_transforms, n, k)\n",
        "    mdl.compile('adam',\n",
        "                'categorical_crossentropy',\n",
        "                ['acc'])\n",
        "\n",
        "    x_train_task = x_train[y_train.flatten() == single_class_ind]\n",
        "    transformations_inds = np.tile(np.arange(transformer.n_transforms), len(x_train_task))\n",
        "    print('Start of the proposed transformation')\n",
        "    x_train_task_transformed = transformer.transform_batch(np.repeat(x_train_task, transformer.n_transforms, axis=0),\n",
        "                                                           transformations_inds)\n",
        "    print('End of the proposed transformation')\n",
        "    batch_size = 128\n",
        "    print('Start of the training')\n",
        "    mdl.fit(x=x_train_task_transformed, y=to_categorical(transformations_inds),\n",
        "            batch_size=batch_size, epochs= 5\n",
        "            )\n",
        "    print('End of the training')\n",
        "\n",
        "    #Estimating the parameters\n",
        "    def calc_approx_alpha_sum(observations):\n",
        "        N = observations.shape[0]\n",
        "        K = observations.shape[1]\n",
        "        f = np.mean(observations, axis=0)\n",
        "        log_obs = (np.nan_to_num(np.log(np.clip(observations,10**(-10),None)), copy=True, nan=10, posinf=10**10, neginf=-(10**10)))\n",
        "\n",
        "        return (N * (K - 1) * (-psi(1))) / (\n",
        "                N * sum(f * np.log(f)) + sum([np.log(sum(observations[j][:K-1])) for j in range(N)]) - sum([sum(f*log_obs[j]) for j in range(N)]) )\n",
        "    def calcul_sum_alpha_beta(observations):\n",
        "      N = observations.shape[0]\n",
        "      K = observations.shape[1]\n",
        "      f = np.mean(np.mean(observations, axis=0))\n",
        "      return (N * (-psi(1))) / (\n",
        "                2*N*(f * np.log(f)) -f* sum([np.log(sum(observations[j][:K-1])) for j in range(N)]) - f*sum([np.log(observations)[j][K-1] for j in range(N)]) )\n",
        "\n",
        "    def inv_psi(y, iters=5):\n",
        "        # initial estimate\n",
        "        cond = y >= -2.22\n",
        "        x = cond * (np.exp(y) + 0.5) + (1 - cond) * -1 / (y - psi(1))\n",
        "\n",
        "        for _ in range(iters):\n",
        "            x = x - (psi(x) - y) / polygamma(1, x)\n",
        "        return x\n",
        "\n",
        "    def fixed_point_beta_louiville_mle(vect_alpha_0, alpha_0, beta_0,lamda_0, y, max_iter=100):\n",
        "        y = np.asarray(y)\n",
        "        #log_y = np.nan_to_num(np.log(np.clip(y,10**(-10),None)), copy=True, nan=1, posinf=10**10, neginf=-(10**10)).mean(axis=0)\n",
        "        N = y.shape[0]\n",
        "        K = y.shape[1]\n",
        "        alpha_old = alpha_0\n",
        "        beta_old = beta_0\n",
        "        lamda_old = lamda_0\n",
        "        vect_alpha_old = vect_alpha_0\n",
        "\n",
        "        for iterr in range(max_iter):\n",
        "          #print('iteration:',iterr)\n",
        "          #calculate alpha\n",
        "          aph = psi(alpha_old + beta_old) + np.log(lamda_old) + (1/N)*np.sum(np.log(np.mean(y,axis=1))) - (1/N)*np.sum([np.log(1 - (1-lamda_old)*np.mean(y[j])) for j in range(N)])\n",
        "          aph = np.nan_to_num(aph, copy=True, nan=10**7, posinf=10**10, neginf=-(10**10))\n",
        "          #print('aph at iteration=',iterr,'=',aph)\n",
        "          alpha_new = inv_psi(np.clip(aph,None,10), iters=5)\n",
        "          alpha_new = np.clip(alpha_new,10**(-10),None)\n",
        "          #print('alpha_new at iter:',iter,'=',alpha_new)\n",
        "\n",
        "          #calculate beta\n",
        "          bet = psi(alpha_old + beta_old) + (1/N)*np.sum(np.log(1-np.mean(y,axis=1))) - (1/N)*np.sum([np.log(1 - (1-lamda_old)*np.mean(y[j])) for j in range(N)])\n",
        "          bet = np.nan_to_num(bet, copy=True, nan=10**7, posinf=10**10, neginf=-(10**10))\n",
        "          #print('bet at iteration=',iterr,'=',bet)\n",
        "          beta_new = inv_psi(np.clip(bet,None,10), iters=5)\n",
        "          beta_new = np.clip(beta_new,10**(-10),None)\n",
        "          #print('beta_new at iter:',iter,'=',beta_new)\n",
        "\n",
        "          #calculate lamda\n",
        "          lamda_new = (N*alpha_old) / np.sum([(alpha_old+beta_old)*np.mean(y[j])/(1-(1-lamda_old)*np.mean(y[j])) for j in range(N)])\n",
        "          lamda_new = np.nan_to_num(lamda_new, copy=True, nan=10**7, posinf=10**10, neginf=-(10**10))\n",
        "          lamda_new = np.clip(lamda_new,10**(-10),None)\n",
        "          #print('lamda_new at iter:',iter,'=',lamda_new)\n",
        "\n",
        "          #calculate the vector alpha\n",
        "          vect_alpha_new = np.asarray([inv_psi(psi(np.sum(vect_alpha_old)) + (1/N)*np.sum([np.log(y[j][i]) for j in range(N)]) - (1/N)*np.sum(np.log(np.mean(y,axis=1))) ,iters=5) for i in range(K)])\n",
        "          vect_alpha_new = np.nan_to_num((np.clip(vect_alpha_new,10**(-10),None)), copy=True, nan=10**7, posinf=10**10, neginf=-(10**10))\n",
        "          #print('vect_alpha_new at iter:',iter,'=',vect_alpha_new)\n",
        "          #if np.sqrt(np.sum((vect_alpha_old - vect_alpha_new) ** 2)) < 1e-6 and np.sqrt((alpha_new - alpha_old) ** 2) < 1e-6 and np.sqrt((lamda_new - lamda_old) ** 2) < 1e-6 and np.sqrt((beta_new - beta_old) ** 2) < 1e-6:\n",
        "            #break\n",
        "          #update the parameters\n",
        "          alpha_old = alpha_new\n",
        "          beta_old = beta_new\n",
        "          lamda_old = lamda_new\n",
        "          vect_alpha_old = vect_alpha_new\n",
        "        return vect_alpha_new, alpha_new, beta_new, lamda_new\n",
        "\n",
        "    #Normality Score\n",
        "    def lnbl_normality_score(vect_alpha,alpha,beta,lamda,y):\n",
        "      N = y.shape[0]\n",
        "      K = y.shape[1]\n",
        "      scores = np.asarray([ (alpha-np.sum(vect_alpha))*np.log(np.mean(y[j])) + (beta-1)*np.log(1-np.mean(y[j])) - (alpha+beta)*np.log(1-(1-lamda)*np.mean(y[j])) + np.sum((vect_alpha-1)*np.log(y[j])) for j in range(N)])\n",
        "      scores = np.nan_to_num(scores, copy=True, nan=10**7, posinf=10**10, neginf=-(10**10))\n",
        "      return scores\n",
        "\n",
        "    scores = np.zeros((len(x_test),))\n",
        "    observed_data = x_train_task\n",
        "    for t_ind in range(transformer.n_transforms):   #transformer.n_transforms\n",
        "        #print('transformation:',t_ind)\n",
        "        observed_dirichlet = mdl.predict(transformer.transform_batch(observed_data, [t_ind] * len(observed_data)),\n",
        "                                         batch_size=64)\n",
        "        vect_alpha_0 = 0.05*np.ones(observed_dirichlet.shape[1])\n",
        "        #vect_alpha_0 = np.random.uniform(low=0.1, high=10, size=(observed_dirichlet.shape[1],))\n",
        "        alpha_0 = 1.4\n",
        "        beta_0 = 1.2\n",
        "        lamda_0 = 1.6\n",
        "\n",
        "        mle_vect_alpha_t,mle_alpha_t,mle_beta_t,mle_lamda_t = fixed_point_beta_louiville_mle(vect_alpha_0, alpha_0, beta_0,lamda_0,observed_dirichlet,max_iter=30)\n",
        "        mle_vect_alpha_t=np.nan_to_num(mle_vect_alpha_t, copy=True, nan=10**3, posinf=10**10, neginf=-10*10)\n",
        "        mle_alpha_t= np.nan_to_num(mle_alpha_t, copy=True, nan=10**7, posinf=10**10, neginf=-10*10)\n",
        "        mle_beta_t= np.nan_to_num(mle_beta_t, copy=True, nan=10**7, posinf=10**10, neginf=-10**10)\n",
        "        mle_lamda_t= np.nan_to_num(mle_lamda_t, copy=True, nan=10**7, posinf=10**10, neginf=-10**10)\n",
        "\n",
        "        x_test_p = mdl.predict(transformer.transform_batch(x_test, [t_ind] * len(x_test)),\n",
        "                               batch_size=64)\n",
        "        #print('x_test_p at transformation:',t_ind,'=',x_test_p)\n",
        "        scores += lnbl_normality_score(mle_vect_alpha_t,mle_alpha_t,mle_beta_t,mle_lamda_t, x_test_p)\n",
        "        scores = np.nan_to_num(scores, copy=True, nan=10**7, posinf=10**10, neginf=-(10**10))\n",
        "    #print('scores=',scores)\n",
        "\n",
        "    scores /= transformer.n_transforms\n",
        "    labels = y_test.flatten() == single_class_ind\n",
        "    roc_pr_curve_data(scores, labels)\n",
        "\n",
        "    gpu_q.put(gpu_to_use)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzJ1wKT7OatO"
      },
      "outputs": [],
      "source": [
        "def run_experiments(load_dataset_fn, dataset_name, q, n_classes):\n",
        "    print(\"START _transformations_experiment \")\n",
        "    n_runs = 1\n",
        "    # Transformations\n",
        "    for i in range(n_runs):\n",
        "        print('Run number:',i+1)\n",
        "        processes = [Process(target=_transformations_experiment,\n",
        "                             args=(load_dataset_fn, dataset_name, c, q)) for c in range(n_classes)]\n",
        "        for p in processes:\n",
        "            p.start()\n",
        "        for p in processes:\n",
        "            p.join()\n",
        "    print(\"END _transformations_experiment \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOqhuKeYOrEm"
      },
      "outputs": [],
      "source": [
        "RESULTS_DIR = ''\n",
        "TRAIN_SIZE = 10000\n",
        "VAL_SIZE = 1000\n",
        "BATCH_SIZE = 128\n",
        "N_EPOCHS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_3zJdh_O0XD",
        "outputId": "7f5f119f-8ab1-4e0a-d472-66837cb086e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "START _transformations_experiment \n",
            "Run number: 1\n",
            "training class: 0\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "Wide Residual Network-16-8 created.\n",
            "Start of the proposed transformation\n",
            "End of the proposed transformation\n",
            "Start of the training\n",
            "Epoch 1/5\n",
            "282/282 [==============================] - 75s 191ms/step - loss: 0.9988 - acc: 0.7568\n",
            "Epoch 2/5\n",
            "282/282 [==============================] - 53s 187ms/step - loss: 0.1762 - acc: 0.9431\n",
            "Epoch 3/5\n",
            "282/282 [==============================] - 53s 189ms/step - loss: 0.0641 - acc: 0.9826\n",
            "Epoch 4/5\n",
            "282/282 [==============================] - 54s 191ms/step - loss: 0.0395 - acc: 0.9902\n",
            "Epoch 5/5\n",
            "282/282 [==============================] - 54s 192ms/step - loss: 0.0291 - acc: 0.9940\n",
            "End of the training\n",
            "16/16 [==============================] - 2s 72ms/step\n",
            "16/16 [==============================] - 1s 72ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "roc_auc 0.9024622307939569\n",
            "pr_auc_norm where normal is the positive class 0.7215547515029753\n",
            "pr_auc_norm where anomaly is the positive class 0.9816227513206319\n",
            "training class: 1\n",
            "Wide Residual Network-16-8 created.\n",
            "Start of the proposed transformation\n",
            "End of the proposed transformation\n",
            "Start of the training\n",
            "Epoch 1/5\n",
            "317/317 [==============================] - 77s 196ms/step - loss: 0.7122 - acc: 0.8857\n",
            "Epoch 2/5\n",
            "317/317 [==============================] - 60s 188ms/step - loss: 0.1115 - acc: 0.9681\n",
            "Epoch 3/5\n",
            "317/317 [==============================] - 60s 189ms/step - loss: 0.0586 - acc: 0.9836\n",
            "Epoch 4/5\n",
            "317/317 [==============================] - 60s 190ms/step - loss: 0.0390 - acc: 0.9901\n",
            "Epoch 5/5\n",
            "317/317 [==============================] - 60s 190ms/step - loss: 0.0245 - acc: 0.9948\n",
            "End of the training\n",
            "18/18 [==============================] - 2s 62ms/step\n",
            "16/16 [==============================] - 1s 72ms/step\n",
            "18/18 [==============================] - 1s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "18/18 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "18/18 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "18/18 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "18/18 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "18/18 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "18/18 [==============================] - 0s 26ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "18/18 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "roc_auc 0.9380970542297774\n",
            "pr_auc_norm where normal is the positive class 0.8541233672643391\n",
            "pr_auc_norm where anomaly is the positive class 0.9841949047440381\n",
            "training class: 2\n",
            "Wide Residual Network-16-8 created.\n",
            "Start of the proposed transformation\n",
            "End of the proposed transformation\n",
            "Start of the training\n",
            "Epoch 1/5\n",
            "279/279 [==============================] - 69s 194ms/step - loss: 0.7131 - acc: 0.9135\n",
            "Epoch 2/5\n",
            "279/279 [==============================] - 52s 187ms/step - loss: 0.0262 - acc: 0.9963\n",
            "Epoch 3/5\n",
            "279/279 [==============================] - 53s 189ms/step - loss: 0.0189 - acc: 0.9971\n",
            "Epoch 4/5\n",
            "279/279 [==============================] - 53s 190ms/step - loss: 0.0149 - acc: 0.9981\n",
            "Epoch 5/5\n",
            "279/279 [==============================] - 53s 190ms/step - loss: 0.0147 - acc: 0.9979\n",
            "End of the training\n",
            "16/16 [==============================] - 2s 62ms/step\n",
            "16/16 [==============================] - 1s 72ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 34ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 34ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "roc_auc 0.9888438133874239\n",
            "pr_auc_norm where normal is the positive class 0.9428612456783525\n",
            "pr_auc_norm where anomaly is the positive class 0.9984806261208703\n",
            "training class: 3\n",
            "Wide Residual Network-16-8 created.\n",
            "Start of the proposed transformation\n",
            "End of the proposed transformation\n",
            "Start of the training\n",
            "Epoch 1/5\n",
            "291/291 [==============================] - 69s 190ms/step - loss: 0.6592 - acc: 0.9232\n",
            "Epoch 2/5\n",
            "291/291 [==============================] - 55s 188ms/step - loss: 0.0220 - acc: 0.9971\n",
            "Epoch 3/5\n",
            "291/291 [==============================] - 55s 189ms/step - loss: 0.0091 - acc: 0.9995\n",
            "Epoch 4/5\n",
            "291/291 [==============================] - 55s 190ms/step - loss: 0.0165 - acc: 0.9975\n",
            "Epoch 5/5\n",
            "291/291 [==============================] - 55s 190ms/step - loss: 0.0133 - acc: 0.9982\n",
            "End of the training\n",
            "17/17 [==============================] - 2s 46ms/step\n",
            "16/16 [==============================] - 1s 74ms/step\n",
            "17/17 [==============================] - 0s 29ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "17/17 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "17/17 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "17/17 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "17/17 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "17/17 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "roc_auc 0.9950706952308191\n",
            "pr_auc_norm where normal is the positive class 0.971266511682455\n",
            "pr_auc_norm where anomaly is the positive class 0.9993940255922181\n",
            "training class: 4\n",
            "Wide Residual Network-16-8 created.\n",
            "Start of the proposed transformation\n",
            "End of the proposed transformation\n",
            "Start of the training\n",
            "Epoch 1/5\n",
            "276/276 [==============================] - 67s 194ms/step - loss: 0.7169 - acc: 0.9165\n",
            "Epoch 2/5\n",
            "276/276 [==============================] - 52s 188ms/step - loss: 0.0336 - acc: 0.9931\n",
            "Epoch 3/5\n",
            "276/276 [==============================] - 52s 189ms/step - loss: 0.0217 - acc: 0.9959\n",
            "Epoch 4/5\n",
            "276/276 [==============================] - 52s 190ms/step - loss: 0.0195 - acc: 0.9965\n",
            "Epoch 5/5\n",
            "276/276 [==============================] - 53s 191ms/step - loss: 0.0070 - acc: 0.9999\n",
            "End of the training\n",
            "16/16 [==============================] - 2s 57ms/step\n",
            "16/16 [==============================] - 1s 71ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "roc_auc 0.9894892747701737\n",
            "pr_auc_norm where normal is the positive class 0.9548223670642122\n",
            "pr_auc_norm where anomaly is the positive class 0.9985500463799311\n",
            "training class: 5\n",
            "Wide Residual Network-16-8 created.\n",
            "Start of the proposed transformation\n",
            "End of the proposed transformation\n",
            "Start of the training\n",
            "Epoch 1/5\n",
            "243/243 [==============================] - 62s 196ms/step - loss: 0.8022 - acc: 0.8882\n",
            "Epoch 2/5\n",
            "243/243 [==============================] - 46s 187ms/step - loss: 0.0255 - acc: 0.9970\n",
            "Epoch 3/5\n",
            "243/243 [==============================] - 46s 189ms/step - loss: 0.0171 - acc: 0.9978\n",
            "Epoch 4/5\n",
            "243/243 [==============================] - 46s 189ms/step - loss: 0.0099 - acc: 0.9993\n",
            "Epoch 5/5\n",
            "243/243 [==============================] - 46s 191ms/step - loss: 0.0118 - acc: 0.9988\n",
            "End of the training\n",
            "14/14 [==============================] - 2s 67ms/step\n",
            "16/16 [==============================] - 1s 73ms/step\n",
            "14/14 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "14/14 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "14/14 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "14/14 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "14/14 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "14/14 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "14/14 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 30ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 31ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 31ms/step\n",
            "14/14 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "14/14 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "14/14 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "14/14 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "14/14 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "roc_auc 0.9956440180785839\n",
            "pr_auc_norm where normal is the positive class \n",
            "0.9615478048242178pr_auc_norm where anomaly is the positive class 0.9995837020123638\n",
            "training class: 6\n",
            "Wide Residual Network-16-8 created.\n",
            "Start of the proposed transformation\n",
            "End of the proposed transformation\n",
            "Start of the training\n",
            "Epoch 1/5\n",
            "286/286 [==============================] - 68s 190ms/step - loss: 0.6761 - acc: 0.9248\n",
            "Epoch 2/5\n",
            "286/286 [==============================] - 54s 188ms/step - loss: 0.0305 - acc: 0.9955\n",
            "Epoch 3/5\n",
            "286/286 [==============================] - 54s 189ms/step - loss: 0.0148 - acc: 0.9981\n",
            "Epoch 4/5\n",
            "286/286 [==============================] - 54s 190ms/step - loss: 0.0150 - acc: 0.9979\n",
            "Epoch 5/5\n",
            "286/286 [==============================] - 54s 190ms/step - loss: 0.0070 - acc: 0.9999\n",
            "End of the training\n",
            "16/16 [==============================] - 2s 75ms/step\n",
            "16/16 [==============================] - 1s 70ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 34ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 30ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 30ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 30ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 26ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 34ms/step\n",
            "16/16 [==============================] - 0s 26ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "roc_auc 0.9991816796968438\n",
            "pr_auc_norm where normal is the positive class 0.993339450011769\n",
            "pr_auc_norm where anomaly is the positive class 0.999920937177758\n",
            "training class: 7\n",
            "Wide Residual Network-16-8 created.\n",
            "Start of the proposed transformation\n",
            "End of the proposed transformation\n",
            "Start of the training\n",
            "Epoch 1/5\n",
            "301/301 [==============================] - 75s 199ms/step - loss: 0.6595 - acc: 0.9238\n",
            "Epoch 2/5\n",
            "301/301 [==============================] - 57s 190ms/step - loss: 0.0377 - acc: 0.9914\n",
            "Epoch 3/5\n",
            "301/301 [==============================] - 58s 192ms/step - loss: 0.0279 - acc: 0.9935\n",
            "Epoch 4/5\n",
            "301/301 [==============================] - 58s 193ms/step - loss: 0.0218 - acc: 0.9953\n",
            "Epoch 5/5\n",
            "301/301 [==============================] - 58s 193ms/step - loss: 0.0153 - acc: 0.9972\n",
            "End of the training\n",
            "17/17 [==============================] - 2s 69ms/step\n",
            "16/16 [==============================] - 1s 74ms/step\n",
            "17/17 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "17/17 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "17/17 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "17/17 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "17/17 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 26ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "17/17 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "17/17 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "roc_auc 0.9603134564288838\n",
            "pr_auc_norm where normal is the positive class 0.7621453566802666\n",
            "pr_auc_norm where anomaly is the positive class 0.9952954067019635\n",
            "training class: 8\n",
            "Wide Residual Network-16-8 created.\n",
            "Start of the proposed transformation\n",
            "End of the proposed transformation\n",
            "Start of the training\n",
            "Epoch 1/5\n",
            "266/266 [==============================] - 67s 196ms/step - loss: 0.8892 - acc: 0.8300\n",
            "Epoch 2/5\n",
            "266/266 [==============================] - 50s 190ms/step - loss: 0.0829 - acc: 0.9768\n",
            "Epoch 3/5\n",
            "266/266 [==============================] - 51s 191ms/step - loss: 0.0367 - acc: 0.9917\n",
            "Epoch 4/5\n",
            "266/266 [==============================] - 51s 192ms/step - loss: 0.0226 - acc: 0.9952\n",
            "Epoch 5/5\n",
            "266/266 [==============================] - 51s 193ms/step - loss: 0.0255 - acc: 0.9949\n",
            "End of the training\n",
            "15/15 [==============================] - 1s 77ms/step\n",
            "16/16 [==============================] - 1s 71ms/step\n",
            "15/15 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "15/15 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "15/15 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 34ms/step\n",
            "15/15 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 30ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 31ms/step\n",
            "15/15 [==============================] - 0s 29ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 31ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "15/15 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 30ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "15/15 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 34ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "15/15 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "roc_auc 0.8683629546491695\n",
            "pr_auc_norm where normal is the positive class 0.6511264260842263\n",
            "pr_auc_norm where anomaly is the positive class 0.9760732868685105\n",
            "training class: 9\n",
            "Wide Residual Network-16-8 created.\n",
            "Start of the proposed transformation\n",
            "End of the proposed transformation\n",
            "Start of the training\n",
            "Epoch 1/5\n",
            "276/276 [==============================] - 66s 188ms/step - loss: 0.6970 - acc: 0.9105\n",
            "Epoch 2/5\n",
            "276/276 [==============================] - 52s 187ms/step - loss: 0.0239 - acc: 0.9970\n",
            "Epoch 3/5\n",
            "276/276 [==============================] - 52s 189ms/step - loss: 0.0176 - acc: 0.9974\n",
            "Epoch 4/5\n",
            "276/276 [==============================] - 52s 190ms/step - loss: 0.0209 - acc: 0.9969\n",
            "Epoch 5/5\n",
            "276/276 [==============================] - 52s 190ms/step - loss: 0.0075 - acc: 0.9998\n",
            "End of the training\n",
            "16/16 [==============================] - 2s 51ms/step\n",
            "16/16 [==============================] - 1s 71ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 34ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 30ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 34ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 31ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 28ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 32ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 1s 33ms/step\n",
            "16/16 [==============================] - 0s 27ms/step\n",
            "16/16 [==============================] - 0s 32ms/step\n",
            "roc_auc 0.9870954863557372\n",
            "pr_auc_norm where normal is the positive class 0.9318357333262213\n",
            "pr_auc_norm where anomaly is the positive class 0.998568477704175\n",
            "END _transformations_experiment \n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    freeze_support()\n",
        "    N_GPUS = 1\n",
        "    man = Manager()\n",
        "    q = man.Queue(N_GPUS)\n",
        "    for g in range(N_GPUS):\n",
        "        q.put(str(g))\n",
        "\n",
        "    experiments_list = [\n",
        "        #(load_fashion_mnist, 'fashion-mnist', 10),\n",
        "        #(load_cifar10, 'cifar10', 10),\n",
        "        (load_mnist, 'mnist', 10),\n",
        "    ]\n",
        "\n",
        "    for data_load_fn, dataset_name, n_classes in experiments_list:\n",
        "        run_experiments(data_load_fn, dataset_name, q, n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MsbRJ-OPLpo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}