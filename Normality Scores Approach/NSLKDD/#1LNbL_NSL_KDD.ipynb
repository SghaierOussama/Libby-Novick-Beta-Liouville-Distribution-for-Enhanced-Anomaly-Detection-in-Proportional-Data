{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3ZndcLkk56W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from scipy.special import psi, polygamma\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WH9oDi_RwYeD"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "from keras.layers import Input, Dense, Dropout, Activation, Lambda\n",
        "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
        "from keras.layers import LSTM, GRU, SimpleRNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDj3a6Qwz2Id"
      },
      "outputs": [],
      "source": [
        "#This function is used to encode a vector as follows:\n",
        "'''\n",
        "when the vector element == 'normal', it will be transformed to 1 and to 0 if not\n",
        "'''\n",
        "def encoding(y):\n",
        "  enc = [1 if y[i]=='normal' else 0 for i in range(len(y))]\n",
        "  return pd.DataFrame(enc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S1SbsExOQTY"
      },
      "source": [
        "##**Load the Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egAkHKomKE_1"
      },
      "outputs": [],
      "source": [
        "def load_data(path_train, path_test):\n",
        "  train = pd.read_csv(path_train)  #load the train data\n",
        "  test = pd.read_csv(path_test)  #load the test data\n",
        "\n",
        "  train_normal = pd.DataFrame(train[train['connection_type'] == 'normal'])   #train samples which have normal as label\n",
        "  train_attack = pd.DataFrame(train[train['connection_type'] != 'normal'])   #train samples which have attack as label\n",
        "  test_normal = pd.DataFrame(test[test['connection_type'] == 'normal'])      #test samples which have normal as label\n",
        "  test_attack = pd.DataFrame(test[test['connection_type'] != 'normal'])      #test samples which have attack as label\n",
        "\n",
        "  nbr_of_normal_train_samples = 10000  #number of the normal samples that we will work with in the train data\n",
        "  nbr_of_attack_train_samples = 10000  #number of the attack samples that we will work with in the train data\n",
        "  nbr_of_normal_test_samples = 2500   #number of the normal samples that we will work with in the test data\n",
        "  nbr_of_attack_test_samples = 2500    #number of the attack samples that we will work with in the test data\n",
        "\n",
        "  tf.random.set_seed(0)\n",
        "  train_normal = pd.DataFrame(train_normal[:nbr_of_normal_train_samples])\n",
        "  train_attack = pd.DataFrame(train_attack[:nbr_of_attack_train_samples])\n",
        "  test_normal = pd.DataFrame(test_normal[:nbr_of_normal_test_samples])\n",
        "  test_attack = pd.DataFrame(test_attack[:nbr_of_attack_test_samples])\n",
        "\n",
        "  #concatenate the normal samples and the attack samples into a single data with the desired number of normal samples and attack samples for both train and test\n",
        "  train = pd.DataFrame(pd.concat([train_normal,train_attack]))\n",
        "  test = pd.DataFrame(pd.concat([test_normal,test_attack]))\n",
        "\n",
        "  train = train.reset_index(drop = True)   #reset index for the train data\n",
        "  test = test.reset_index(drop = True)     #reset index for the test data\n",
        "\n",
        "  #split the train and test into features data and labels\n",
        "  x_train = pd.DataFrame(train.iloc[:,:41])\n",
        "  y_train = pd.Series(train.iloc[:,41])\n",
        "  x_test = pd.DataFrame(test.iloc[:,:41])\n",
        "  y_test = pd.Series(test.iloc[:,41])\n",
        "\n",
        "  #use the command 'get_dummies' to eliminate the categorical features\n",
        "  x_train = pd.get_dummies(x_train)\n",
        "  x_test = pd.get_dummies(x_test)\n",
        "\n",
        "  #join x_train and x_test as both of them they haven't the same shape after 'get_dummies' command\n",
        "  x_train , x_test = x_train.align(x_test, join = 'inner', axis = 1)\n",
        "\n",
        "  #encode both y_train and y_test\n",
        "  y_train = encoding(y_train)\n",
        "  y_test = encoding(y_test)\n",
        "\n",
        "  #Normalize the data\n",
        "  scaler = Normalizer().fit(x_train)\n",
        "  x_train = scaler.transform(x_train)\n",
        "\n",
        "  scaler = Normalizer().fit(x_test)\n",
        "  x_test = scaler.transform(x_test)\n",
        "\n",
        "  #transform y_train and y_test into arrays\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  return (x_train , y_train) , (x_test , y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHw1vTmnOoyJ"
      },
      "source": [
        "##**Transformtions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRLQfo3qMKRf"
      },
      "outputs": [],
      "source": [
        "def transformation(input_dim, h_dim,):\n",
        "  input = Input(shape=(input_dim,))\n",
        "  layer1 = Dense(h_dim, use_bias = False, activation='relu')(input)\n",
        "  output = Dense(input_dim, use_bias = False, activation='sigmoid')(layer1)\n",
        "\n",
        "  transformer = Model(input, output)\n",
        "  return transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ1z4QCwMRy0"
      },
      "outputs": [],
      "source": [
        "def create_transformations_list(input_dim, h_dim, num_trans):\n",
        "  trans_list = []\n",
        "  for i in range(num_trans):\n",
        "    trans = transformation(input_dim ,h_dim[i])\n",
        "    trans_list.append(trans)\n",
        "  return trans_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKt6FFlLF6zm"
      },
      "source": [
        "##**Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "640FCxy3hh6w"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "def AutoEncoder(input_shape):\n",
        "  input = Input(shape=(input_shape,))\n",
        "  layer1 = Dense(128, use_bias = False, activation='relu')(input)\n",
        "  layer2 = Dense(64, use_bias = False, activation='relu')(layer1)\n",
        "  encoded = Dense(32, use_bias = False, activation='relu')(layer2)\n",
        "  layer3 = Dense(64, use_bias = False, activation='relu')(encoded)\n",
        "  layer4 = Dense(128, use_bias = False, activation='relu')(layer3)\n",
        "  decoded = Dense(input_shape, use_bias = False, activation='relu')(layer4)\n",
        "\n",
        "  autoencoder = Model(input, decoded)\n",
        "  return autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG-9hN7C0Yw7"
      },
      "outputs": [],
      "source": [
        "def classifier(input_shape,lstm_output_size,nbr_of_classes):\n",
        "  model = Sequential()\n",
        "  model.add(Convolution1D(128, 3, padding=\"same\",activation=\"relu\",input_shape=(input_shape, 1)))\n",
        "  model.add(MaxPooling1D(pool_size=3))\n",
        "  model.add(LSTM(lstm_output_size))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Dense(nbr_of_classes, activation=\"softmax\"))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKX5lswmRO8Z"
      },
      "source": [
        "##**Experiments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "498PoEZ2y6ol"
      },
      "outputs": [],
      "source": [
        "#Estimating the parameters\n",
        "def inv_psi(y, iters=5):\n",
        "  # initial estimate\n",
        "  cond = y >= -2.22\n",
        "  x = cond * (np.exp(y) + 0.5) + (1 - cond) * -1 / (y - psi(1))\n",
        "\n",
        "  for _ in range(iters):\n",
        "      x = x - (psi(x) - y) / polygamma(1, x)\n",
        "  return x\n",
        "\n",
        "def fixed_point_ln_beta_louiville_mle(vect_alpha_0, alpha_0, beta_0,lamda_0, y, max_iter=50):\n",
        "  y = np.asarray(y)\n",
        "  (N,K) = (y.shape[0], y.shape[1])\n",
        "  alpha_old = alpha_0\n",
        "  beta_old = beta_0\n",
        "  lamda_old = lamda_0\n",
        "  vect_alpha_old = vect_alpha_0\n",
        "\n",
        "  for iterr in range(max_iter):\n",
        "    aph = psi(alpha_old + beta_old) + np.log(lamda_old) + (1/N)*np.sum(np.log(np.mean(y,axis=1))) - (1/N)*np.sum([np.abs(np.log(1 - (1-lamda_old)*np.mean(y[j]))) for j in range(N)])\n",
        "    aph = np.nan_to_num(aph, copy=True, nan=10**7, posinf=10**10, neginf=-(10**10))\n",
        "    alpha_new = inv_psi(np.clip(aph,None,10), iters=5)\n",
        "    alpha_new = np.clip(alpha_new,10**(-10),None)\n",
        "\n",
        "    #calculate beta\n",
        "    bet = psi(alpha_old + beta_old) + (1/N)*np.sum(np.log(np.clip(1-np.mean(y,axis=1),10**(-10),None))) - (1/N)*np.sum([np.log(np.abs(1 - (1-lamda_old)*np.mean(y[j]))) for j in range(N)])\n",
        "    bet = np.nan_to_num(bet, copy=True, nan=10**7, posinf=10**10, neginf=-(10**10))\n",
        "    beta_new = inv_psi(np.clip(bet,None,10), iters=5)\n",
        "    beta_new = np.clip(beta_new,10**(-10),None)\n",
        "\n",
        "    #calculate lamda\n",
        "    lamda_new = (N*alpha_old) / np.sum([(alpha_old+beta_old)*np.mean(y[j])/(1-(1-lamda_old)*np.mean(y[j])) for j in range(N)])\n",
        "    lamda_new = np.nan_to_num(lamda_new, copy=True, nan=10**7, posinf=10**10, neginf=-(10**10))\n",
        "    lamda_new = np.clip(lamda_new,10**(-10),None)\n",
        "\n",
        "    #calculate the vector alpha\n",
        "    vect_alpha_new = np.asarray([inv_psi(psi(np.sum(vect_alpha_old)) + (1/N)*np.sum([np.log(y[j][i]) for j in range(N)]) - (1/N)*np.sum(np.mean(y,axis=1)) ,iters=5) for i in range(K)])\n",
        "    vect_alpha_new = np.nan_to_num((np.clip(vect_alpha_new,10**(-10),None)), copy=True, nan=10**7, posinf=10**10, neginf=-(10**10))\n",
        "\n",
        "    #Update the parameters\n",
        "    alpha_old = alpha_new\n",
        "    beta_old = beta_new\n",
        "    lamda_old = lamda_new\n",
        "    vect_alpha_old = vect_alpha_new\n",
        "  return vect_alpha_new, alpha_new, beta_new, lamda_new\n",
        "\n",
        "#Normality Score\n",
        "def lnbl_normality_score(vect_alpha,alpha,beta,lamda,y):\n",
        "  (N,K) = (y.shape[0], y.shape[1])\n",
        "  scores = np.asarray([ (alpha-np.sum(vect_alpha))*np.log(np.mean(y[j])) + (beta-1)*np.log(np.clip(1-np.mean(y[j]),10**(-10),None)) - (alpha+beta)*np.log(np.abs(1-(1-lamda)*np.mean(y[j]))) + np.sum((vect_alpha-1)*np.log(y[j])) for j in range(N)])\n",
        "  scores = np.nan_to_num(scores, copy=True, nan=10**7, posinf=10**10, neginf=-(10**10))\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT9cPrFtT1pk"
      },
      "source": [
        "This cell for calculating our metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIrRgQ1-TytJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
        "def roc_pr_curve_data(scores, labels):\n",
        "    scores = scores.flatten()\n",
        "    labels = labels.flatten()\n",
        "\n",
        "    scores_pos = scores[labels == 1]\n",
        "    scores_neg = scores[labels != 1]\n",
        "\n",
        "    truth = np.concatenate((np.zeros_like(scores_neg), np.ones_like(scores_pos)))\n",
        "    preds = np.concatenate((scores_neg, scores_pos))\n",
        "    fpr, tpr, roc_thresholds = roc_curve(truth, preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc\",roc_auc)\n",
        "\n",
        "    # pr curve where \"normal\" is the positive class\n",
        "    precision_norm, recall_norm, pr_thresholds_norm = precision_recall_curve(truth, preds)\n",
        "    pr_auc_norm = auc(recall_norm, precision_norm)\n",
        "    print(\"pr_auc_norm where normal is the positive class\",pr_auc_norm)\n",
        "\n",
        "    # pr curve where \"anomaly\" is the positive class\n",
        "    precision_anom, recall_anom, pr_thresholds_anom = precision_recall_curve(truth, -preds, pos_label=0)\n",
        "    pr_auc_anom = auc(recall_anom, precision_anom)\n",
        "    print(\"pr_auc_norm where anomaly is the positive class\",pr_auc_anom)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the data\n",
        "path_train = \"/content/train_kdd_nsl.csv\"\n",
        "path_test = \"/content/test_kdd_nsl.csv\"\n",
        "(x_train, y_train) , (x_test, y_test) = load_data(path_train, path_test)"
      ],
      "metadata": {
        "id": "7BMeoBBfCb4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbEpN2nARoMD"
      },
      "outputs": [],
      "source": [
        "def experiment(single_class_ind):\n",
        "  print(\"Class \",single_class_ind)\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "  #Transform the data\n",
        "  print('Start of transformations for class',single_class_ind)\n",
        "  x_train_task = x_train[y_train.flatten() == single_class_ind]\n",
        "  num_trans = 10\n",
        "  h_dim = [10,20,30,40,50,60,70,80,90,100]\n",
        "  transformations_inds = np.tile(np.arange(num_trans), len(x_train_task))\n",
        "  transformations_list = create_transformations_list(x_train_task.shape[1], h_dim, num_trans)\n",
        "  data_each_transformation = []\n",
        "  for transformation in transformations_list:\n",
        "    print('transformation ', transformations_list.index(transformation))\n",
        "    transformation.compile(loss=\"mse\", optimizer=\"adamax\",metrics=['accuracy'])\n",
        "    transformation.fit(x_train_task, x_train_task, batch_size=64, epochs= 5)\n",
        "    data_trans = transformation.predict(x_train_task)\n",
        "    data_each_transformation.append(data_trans)\n",
        "  data_transformed = pd.concat([pd.DataFrame(data_each_transformation[i]) for i in range(len(data_each_transformation))])\n",
        "  data_transformed = data_transformed.reset_index(drop = True)\n",
        "  data_organised = []\n",
        "  for i in range(len(x_train_task)):\n",
        "    data_trans_k = data_transformed.iloc[i::len(x_train_task),:]\n",
        "    data_organised.append(data_trans_k)\n",
        "  x_train_task_transformed = pd.concat([pd.DataFrame(data_organised[i]) for i in range(len(data_organised))])\n",
        "  x_train_task_transformed = x_train_task_transformed.reset_index(drop = True)\n",
        "  print('End of transformations for class',single_class_ind)\n",
        "\n",
        "\n",
        "  #Encode the data\n",
        "  print('Start of Encoding for class',single_class_ind)\n",
        "  autoencoder = AutoEncoder(x_train_task_transformed.shape[1])\n",
        "  autoencoder.compile(optimizer='adam', loss='mse')\n",
        "  autoencoder.fit(x_train_task_transformed, x_train_task_transformed, epochs=3, batch_size=64)\n",
        "  x_train_task_encoded = autoencoder.predict(x_train_task_transformed)\n",
        "  print('End of Encoding for class',single_class_ind)\n",
        "\n",
        "  #Create and train the classifier\n",
        "  cls = classifier(x_train_task_transformed.shape[1],70,len(h_dim))\n",
        "  cls.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
        "  cls.fit(x=x_train_task_encoded, y=to_categorical(transformations_inds),\n",
        "            batch_size=64, epochs= 75)    #N_EPOCHS\n",
        "\n",
        "  scores = np.zeros((len(x_test),))\n",
        "  observed_data = x_train_task\n",
        "  for t_ind in range(num_trans):\n",
        "    observed_data_transformed = data_each_transformation[t_ind]\n",
        "    observed_data_autoencoded = autoencoder.predict(observed_data_transformed)\n",
        "    observed_dirichlet = cls.predict(observed_data_autoencoded)\n",
        "\n",
        "    vect_alpha_0 = 0.05*np.ones(observed_dirichlet.shape[1])\n",
        "    alpha_0 = 0.5\n",
        "    beta_0 = 0.4\n",
        "    lamda_0 = 0.5\n",
        "\n",
        "    mle_vect_alpha_t,mle_alpha_t,mle_beta_t,mle_lamda_t = fixed_point_ln_beta_louiville_mle(vect_alpha_0, alpha_0, beta_0,lamda_0,observed_dirichlet,max_iter=50)\n",
        "    mle_vect_alpha_t=np.nan_to_num(mle_vect_alpha_t, copy=True, nan=1, posinf=10**10, neginf=-10*10)\n",
        "    mle_alpha_t= np.nan_to_num(mle_alpha_t, copy=True, nan=10**3, posinf=10**10, neginf=-10*10)\n",
        "    mle_beta_t= np.nan_to_num(mle_beta_t, copy=True, nan=10**3, posinf=10**10, neginf=-10**10)\n",
        "    mle_lamda_t= np.nan_to_num(mle_lamda_t, copy=True, nan=10**3, posinf=10**10, neginf=-10**10)\n",
        "\n",
        "    x_test_p_transformed = transformations_list[t_ind].predict(x_test)\n",
        "    x_test_p_autoencoded = autoencoder.predict(x_test_p_transformed)\n",
        "    x_test_p = cls.predict(x_test_p_autoencoded)\n",
        "    x_test_p = np.nan_to_num(x_test_p, copy=True, nan=0.0001, posinf=None, neginf=None)\n",
        "    scores += lnbl_normality_score(mle_vect_alpha_t,mle_alpha_t,mle_beta_t,mle_lamda_t, x_test_p)\n",
        "\n",
        "  scores /= num_trans\n",
        "  labels = y_test.flatten() == single_class_ind\n",
        "  roc_pr_curve_data(scores, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY4HywQbVeS_",
        "outputId": "bc7bc220-e15a-47ef-b808-121e20479af2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class  0\n",
            "Start of transformations for class 0\n",
            "transformation  0\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 2ms/step - loss: 0.2331 - accuracy: 0.1594\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.1606 - accuracy: 0.6343\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0665 - accuracy: 0.6343\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0260 - accuracy: 0.6343\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.6343\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "transformation  1\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 3ms/step - loss: 0.2155 - accuracy: 0.3657\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0985 - accuracy: 0.6343\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.6343\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0103 - accuracy: 0.6343\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0062 - accuracy: 0.6343\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "transformation  2\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 2ms/step - loss: 0.2028 - accuracy: 0.4848\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0691 - accuracy: 0.6343\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 0.6343\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 0.6343\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.6343\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "transformation  3\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 2ms/step - loss: 0.1790 - accuracy: 0.4481\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0396 - accuracy: 0.6343\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 0.6343\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.6343\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 0.6398\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "transformation  4\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 2ms/step - loss: 0.1719 - accuracy: 0.5819\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0306 - accuracy: 0.6343\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 0.6343\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.6384\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.6852\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "transformation  5\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 2ms/step - loss: 0.1666 - accuracy: 0.5782\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0252 - accuracy: 0.6343\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0064 - accuracy: 0.6352\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0036 - accuracy: 0.6455\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0025 - accuracy: 0.7200\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "transformation  6\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 2ms/step - loss: 0.1455 - accuracy: 0.5044\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0181 - accuracy: 0.6343\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0056 - accuracy: 0.6355\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0033 - accuracy: 0.7002\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.7337\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "transformation  7\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 2ms/step - loss: 0.1504 - accuracy: 0.5532\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0177 - accuracy: 0.6344\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 0.6560\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.7239\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 0.7462\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "transformation  8\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 4ms/step - loss: 0.1473 - accuracy: 0.5450\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 1s 3ms/step - loss: 0.0157 - accuracy: 0.6344\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 1s 3ms/step - loss: 0.0046 - accuracy: 0.6719\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.7155\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0018 - accuracy: 0.7514\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "transformation  9\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 2ms/step - loss: 0.1375 - accuracy: 0.5852\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0124 - accuracy: 0.6414\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.6975\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.7362\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 0.7807\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "End of transformations for class 0\n",
            "Start of Encoding for class 0\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 4.9147e-04\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.5494e-04\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.4527e-04\n",
            "3125/3125 [==============================] - 5s 2ms/step\n",
            "End of Encoding for class 0\n",
            "Epoch 1/75\n",
            "1563/1563 [==============================] - 122s 77ms/step - loss: 1.6890 - accuracy: 0.3820\n",
            "Epoch 2/75\n",
            "1563/1563 [==============================] - 120s 77ms/step - loss: 0.8007 - accuracy: 0.6935\n",
            "Epoch 3/75\n",
            "1563/1563 [==============================] - 122s 78ms/step - loss: 0.6030 - accuracy: 0.7641\n",
            "Epoch 4/75\n",
            "1563/1563 [==============================] - 116s 74ms/step - loss: 0.4996 - accuracy: 0.7990\n",
            "Epoch 5/75\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 0.4739 - accuracy: 0.8120\n",
            "Epoch 6/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.4355 - accuracy: 0.8288\n",
            "Epoch 7/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.3864 - accuracy: 0.8462\n",
            "Epoch 8/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.3465 - accuracy: 0.8641\n",
            "Epoch 9/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.3143 - accuracy: 0.8781\n",
            "Epoch 10/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.3037 - accuracy: 0.8839\n",
            "Epoch 11/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.2839 - accuracy: 0.8914\n",
            "Epoch 12/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.2822 - accuracy: 0.8917\n",
            "Epoch 13/75\n",
            "1563/1563 [==============================] - 120s 77ms/step - loss: 0.2616 - accuracy: 0.9027\n",
            "Epoch 14/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.2457 - accuracy: 0.9090\n",
            "Epoch 15/75\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 0.2632 - accuracy: 0.9031\n",
            "Epoch 16/75\n",
            "1563/1563 [==============================] - 116s 75ms/step - loss: 0.2291 - accuracy: 0.9159\n",
            "Epoch 17/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.2251 - accuracy: 0.9187\n",
            "Epoch 18/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.1931 - accuracy: 0.9299\n",
            "Epoch 19/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.1718 - accuracy: 0.9370\n",
            "Epoch 20/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.1696 - accuracy: 0.9390\n",
            "Epoch 21/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.1608 - accuracy: 0.9411\n",
            "Epoch 22/75\n",
            "1563/1563 [==============================] - 115s 74ms/step - loss: 0.1466 - accuracy: 0.9468\n",
            "Epoch 23/75\n",
            "1563/1563 [==============================] - 116s 74ms/step - loss: 0.1670 - accuracy: 0.9439\n",
            "Epoch 24/75\n",
            "1563/1563 [==============================] - 115s 73ms/step - loss: 0.1531 - accuracy: 0.9472\n",
            "Epoch 25/75\n",
            "1563/1563 [==============================] - 116s 74ms/step - loss: 0.1289 - accuracy: 0.9549\n",
            "Epoch 26/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.1281 - accuracy: 0.9549\n",
            "Epoch 27/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.1325 - accuracy: 0.9542\n",
            "Epoch 28/75\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 0.1180 - accuracy: 0.9592\n",
            "Epoch 29/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.1340 - accuracy: 0.9554\n",
            "Epoch 30/75\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 0.1252 - accuracy: 0.9569\n",
            "Epoch 31/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.1030 - accuracy: 0.9641\n",
            "Epoch 32/75\n",
            "1563/1563 [==============================] - 116s 74ms/step - loss: 0.1212 - accuracy: 0.9601\n",
            "Epoch 33/75\n",
            "1563/1563 [==============================] - 116s 74ms/step - loss: 0.0928 - accuracy: 0.9678\n",
            "Epoch 34/75\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 0.1048 - accuracy: 0.9651\n",
            "Epoch 35/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.0921 - accuracy: 0.9683\n",
            "Epoch 36/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.1031 - accuracy: 0.9661\n",
            "Epoch 37/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.0954 - accuracy: 0.9687\n",
            "Epoch 38/75\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 0.0772 - accuracy: 0.9735\n",
            "Epoch 39/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.0856 - accuracy: 0.9719\n",
            "Epoch 40/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.0936 - accuracy: 0.9698\n",
            "Epoch 41/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.0907 - accuracy: 0.9702\n",
            "Epoch 42/75\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 0.0773 - accuracy: 0.9740\n",
            "Epoch 43/75\n",
            "1563/1563 [==============================] - 115s 74ms/step - loss: 0.0889 - accuracy: 0.9713\n",
            "Epoch 44/75\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 0.0871 - accuracy: 0.9703\n",
            "Epoch 45/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.0725 - accuracy: 0.9763\n",
            "Epoch 46/75\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 0.0733 - accuracy: 0.9754\n",
            "Epoch 47/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.0748 - accuracy: 0.9746\n",
            "Epoch 48/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.0722 - accuracy: 0.9758\n",
            "Epoch 49/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.0724 - accuracy: 0.9766\n",
            "Epoch 50/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.0861 - accuracy: 0.9733\n",
            "Epoch 51/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.0666 - accuracy: 0.9773\n",
            "Epoch 52/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.0675 - accuracy: 0.9777\n",
            "Epoch 53/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.0631 - accuracy: 0.9789\n",
            "Epoch 54/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.0671 - accuracy: 0.9776\n",
            "Epoch 55/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.0886 - accuracy: 0.9727\n",
            "Epoch 56/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.0690 - accuracy: 0.9783\n",
            "Epoch 57/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.0554 - accuracy: 0.9812\n",
            "Epoch 58/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.0609 - accuracy: 0.9791\n",
            "Epoch 59/75\n",
            "1563/1563 [==============================] - 120s 77ms/step - loss: 0.0784 - accuracy: 0.9755\n",
            "Epoch 60/75\n",
            "1563/1563 [==============================] - 120s 77ms/step - loss: 0.0609 - accuracy: 0.9800\n",
            "Epoch 61/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.0606 - accuracy: 0.9798\n",
            "Epoch 62/75\n",
            "1563/1563 [==============================] - 120s 77ms/step - loss: 0.0541 - accuracy: 0.9819\n",
            "Epoch 63/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.0568 - accuracy: 0.9810\n",
            "Epoch 64/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.0536 - accuracy: 0.9820\n",
            "Epoch 65/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.0494 - accuracy: 0.9836\n",
            "Epoch 66/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.0498 - accuracy: 0.9830\n",
            "Epoch 67/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.0484 - accuracy: 0.9837\n",
            "Epoch 68/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.0551 - accuracy: 0.9822\n",
            "Epoch 69/75\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 0.0480 - accuracy: 0.9847\n",
            "Epoch 70/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.0488 - accuracy: 0.9833\n",
            "Epoch 71/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.0568 - accuracy: 0.9815\n",
            "Epoch 72/75\n",
            "1563/1563 [==============================] - 120s 77ms/step - loss: 0.0404 - accuracy: 0.9865\n",
            "Epoch 73/75\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 0.0457 - accuracy: 0.9846\n",
            "Epoch 74/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.0513 - accuracy: 0.9836\n",
            "Epoch 75/75\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 0.0421 - accuracy: 0.9862\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "313/313 [==============================] - 4s 13ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 2s 15ms/step\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "313/313 [==============================] - 4s 13ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 2s 13ms/step\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "313/313 [==============================] - 4s 13ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 2s 14ms/step\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "313/313 [==============================] - 4s 13ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 2s 15ms/step\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "313/313 [==============================] - 4s 13ms/step\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 3s 17ms/step\n",
            "313/313 [==============================] - 0s 2ms/step\n",
            "313/313 [==============================] - 4s 13ms/step\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "157/157 [==============================] - 2s 14ms/step\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "313/313 [==============================] - 4s 13ms/step\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 3s 17ms/step\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "313/313 [==============================] - 4s 13ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "157/157 [==============================] - 2s 13ms/step\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "313/313 [==============================] - 4s 13ms/step\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "157/157 [==============================] - 2s 13ms/step\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "313/313 [==============================] - 4s 13ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 2s 13ms/step\n",
            "roc_auc 0.9033191200000001\n",
            "pr_auc_norm where normal is the positive class 0.9085640001727753\n",
            "pr_auc_norm where anomaly is the positive class 0.8852378097592042\n",
            "Class  1\n",
            "Start of transformations for class 1\n",
            "transformation  0\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 2ms/step - loss: 0.2244 - accuracy: 0.1449\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.1375 - accuracy: 0.5341\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0516 - accuracy: 0.5341\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0207 - accuracy: 0.5341\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0116 - accuracy: 0.5341\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "transformation  1\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 2ms/step - loss: 0.2157 - accuracy: 0.1883\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0983 - accuracy: 0.5341\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0254 - accuracy: 0.5394\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0110 - accuracy: 0.5845\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 0.6228\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "transformation  2\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 3ms/step - loss: 0.2022 - accuracy: 0.4664\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0663 - accuracy: 0.5341\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0160 - accuracy: 0.5623\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0078 - accuracy: 0.6341\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 0.6988\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "transformation  3\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 2ms/step - loss: 0.1728 - accuracy: 0.4319\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0355 - accuracy: 0.5341\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0101 - accuracy: 0.5455\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 0.6573\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.7215\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "transformation  4\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 2ms/step - loss: 0.1778 - accuracy: 0.3610\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0336 - accuracy: 0.5450\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0086 - accuracy: 0.6731\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 0.7616\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.8260\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "transformation  5\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 2ms/step - loss: 0.1647 - accuracy: 0.4383\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.6130\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 0.7467\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.7987\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.8165\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "transformation  6\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 3ms/step - loss: 0.1573 - accuracy: 0.4787\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.6426\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 1s 3ms/step - loss: 0.0064 - accuracy: 0.7660\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0038 - accuracy: 0.8247\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.8474\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "transformation  7\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 2ms/step - loss: 0.1430 - accuracy: 0.3634\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0171 - accuracy: 0.6193\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 0.7681\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.8641\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.8947\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "transformation  8\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 2ms/step - loss: 0.1351 - accuracy: 0.5192\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0147 - accuracy: 0.6741\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 0.8129\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.8674\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 0.8979\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "transformation  9\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 1s 2ms/step - loss: 0.1327 - accuracy: 0.4446\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0132 - accuracy: 0.6532\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.7529\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.7850\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.7998\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "End of transformations for class 1\n",
            "Start of Encoding for class 1\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 4.1996e-04\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 3.1954e-04\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 3.0697e-04\n",
            "3125/3125 [==============================] - 5s 1ms/step\n",
            "End of Encoding for class 1\n",
            "Epoch 1/75\n",
            "1563/1563 [==============================] - 123s 77ms/step - loss: 1.3992 - accuracy: 0.4367\n",
            "Epoch 2/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.8005 - accuracy: 0.6700\n",
            "Epoch 3/75\n",
            "1563/1563 [==============================] - 120s 77ms/step - loss: 0.6739 - accuracy: 0.7206\n",
            "Epoch 4/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.5900 - accuracy: 0.7597\n",
            "Epoch 5/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.5560 - accuracy: 0.7788\n",
            "Epoch 6/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.5152 - accuracy: 0.7982\n",
            "Epoch 7/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.4868 - accuracy: 0.8093\n",
            "Epoch 8/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.4241 - accuracy: 0.8354\n",
            "Epoch 9/75\n",
            "1563/1563 [==============================] - 120s 77ms/step - loss: 0.4186 - accuracy: 0.8407\n",
            "Epoch 10/75\n",
            "1563/1563 [==============================] - 116s 74ms/step - loss: 0.3935 - accuracy: 0.8493\n",
            "Epoch 11/75\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 0.3669 - accuracy: 0.8608\n",
            "Epoch 12/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.3586 - accuracy: 0.8647\n",
            "Epoch 13/75\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 0.3340 - accuracy: 0.8739\n",
            "Epoch 14/75\n",
            "1563/1563 [==============================] - 120s 77ms/step - loss: 0.3279 - accuracy: 0.8773\n",
            "Epoch 15/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.3218 - accuracy: 0.8791\n",
            "Epoch 16/75\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 0.3025 - accuracy: 0.8862\n",
            "Epoch 17/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.2926 - accuracy: 0.8894\n",
            "Epoch 18/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.2893 - accuracy: 0.8916\n",
            "Epoch 19/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.2767 - accuracy: 0.8981\n",
            "Epoch 20/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.2628 - accuracy: 0.9013\n",
            "Epoch 21/75\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 0.2551 - accuracy: 0.9046\n",
            "Epoch 22/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.2425 - accuracy: 0.9088\n",
            "Epoch 23/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.2466 - accuracy: 0.9084\n",
            "Epoch 24/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.2240 - accuracy: 0.9161\n",
            "Epoch 25/75\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 0.2014 - accuracy: 0.9255\n",
            "Epoch 26/75\n",
            "1563/1563 [==============================] - 116s 74ms/step - loss: 0.2052 - accuracy: 0.9240\n",
            "Epoch 27/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.2143 - accuracy: 0.9212\n",
            "Epoch 28/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.1895 - accuracy: 0.9296\n",
            "Epoch 29/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.1793 - accuracy: 0.9351\n",
            "Epoch 30/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.1749 - accuracy: 0.9365\n",
            "Epoch 31/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.1849 - accuracy: 0.9337\n",
            "Epoch 32/75\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 0.1652 - accuracy: 0.9413\n",
            "Epoch 33/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.1664 - accuracy: 0.9406\n",
            "Epoch 34/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.1612 - accuracy: 0.9420\n",
            "Epoch 35/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.1656 - accuracy: 0.9414\n",
            "Epoch 36/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.1345 - accuracy: 0.9518\n",
            "Epoch 37/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.1370 - accuracy: 0.9509\n",
            "Epoch 38/75\n",
            "1563/1563 [==============================] - 120s 77ms/step - loss: 0.1372 - accuracy: 0.9518\n",
            "Epoch 39/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.1373 - accuracy: 0.9511\n",
            "Epoch 40/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.1292 - accuracy: 0.9538\n",
            "Epoch 41/75\n",
            "1563/1563 [==============================] - 120s 77ms/step - loss: 0.1196 - accuracy: 0.9577\n",
            "Epoch 42/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.1191 - accuracy: 0.9581\n",
            "Epoch 43/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.1112 - accuracy: 0.9600\n",
            "Epoch 44/75\n",
            "1563/1563 [==============================] - 120s 77ms/step - loss: 0.1154 - accuracy: 0.9590\n",
            "Epoch 45/75\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 0.1039 - accuracy: 0.9623\n",
            "Epoch 46/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.1096 - accuracy: 0.9614\n",
            "Epoch 47/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.1038 - accuracy: 0.9632\n",
            "Epoch 48/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.1037 - accuracy: 0.9627\n",
            "Epoch 49/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.0953 - accuracy: 0.9660\n",
            "Epoch 50/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.1043 - accuracy: 0.9631\n",
            "Epoch 51/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.0955 - accuracy: 0.9663\n",
            "Epoch 52/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.0927 - accuracy: 0.9671\n",
            "Epoch 53/75\n",
            "1563/1563 [==============================] - 120s 77ms/step - loss: 0.0864 - accuracy: 0.9688\n",
            "Epoch 54/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.0889 - accuracy: 0.9679\n",
            "Epoch 55/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.0885 - accuracy: 0.9686\n",
            "Epoch 56/75\n",
            "1563/1563 [==============================] - 120s 76ms/step - loss: 0.0881 - accuracy: 0.9691\n",
            "Epoch 57/75\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 0.0842 - accuracy: 0.9699\n",
            "Epoch 58/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.0937 - accuracy: 0.9673\n",
            "Epoch 59/75\n",
            "1563/1563 [==============================] - 120s 77ms/step - loss: 0.0763 - accuracy: 0.9732\n",
            "Epoch 60/75\n",
            "1563/1563 [==============================] - 116s 74ms/step - loss: 0.0783 - accuracy: 0.9722\n",
            "Epoch 61/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.0779 - accuracy: 0.9721\n",
            "Epoch 62/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.0777 - accuracy: 0.9717\n",
            "Epoch 63/75\n",
            "1563/1563 [==============================] - 120s 77ms/step - loss: 0.0781 - accuracy: 0.9725\n",
            "Epoch 64/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.0765 - accuracy: 0.9727\n",
            "Epoch 65/75\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.0781 - accuracy: 0.9719\n",
            "Epoch 66/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.0737 - accuracy: 0.9737\n",
            "Epoch 67/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.0842 - accuracy: 0.9709\n",
            "Epoch 68/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.0623 - accuracy: 0.9775\n",
            "Epoch 69/75\n",
            "1563/1563 [==============================] - 120s 77ms/step - loss: 0.0742 - accuracy: 0.9741\n",
            "Epoch 70/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.0671 - accuracy: 0.9764\n",
            "Epoch 71/75\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.0696 - accuracy: 0.9756\n",
            "Epoch 72/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.0653 - accuracy: 0.9765\n",
            "Epoch 73/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.0691 - accuracy: 0.9760\n",
            "Epoch 74/75\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 0.0633 - accuracy: 0.9774\n",
            "Epoch 75/75\n",
            "1563/1563 [==============================] - 121s 77ms/step - loss: 0.0612 - accuracy: 0.9782\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "313/313 [==============================] - 5s 14ms/step\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 2s 13ms/step\n",
            "313/313 [==============================] - 0s 2ms/step\n",
            "313/313 [==============================] - 5s 14ms/step\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "157/157 [==============================] - 2s 13ms/step\n",
            "313/313 [==============================] - 0s 2ms/step\n",
            "313/313 [==============================] - 4s 13ms/step\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 2s 13ms/step\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "313/313 [==============================] - 4s 13ms/step\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 2s 13ms/step\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "313/313 [==============================] - 5s 15ms/step\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "157/157 [==============================] - 2s 13ms/step\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "313/313 [==============================] - 5s 15ms/step\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 2s 13ms/step\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "313/313 [==============================] - 4s 14ms/step\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 2s 13ms/step\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "313/313 [==============================] - 4s 13ms/step\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 2s 13ms/step\n",
            "313/313 [==============================] - 0s 2ms/step\n",
            "313/313 [==============================] - 5s 15ms/step\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 2s 13ms/step\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "313/313 [==============================] - 4s 13ms/step\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "157/157 [==============================] - 0s 1ms/step\n",
            "157/157 [==============================] - 2s 13ms/step\n",
            "roc_auc 0.8596732\n",
            "pr_auc_norm where normal is the positive class 0.8527270143798243\n",
            "pr_auc_norm where anomaly is the positive class 0.8689566741230402\n"
          ]
        }
      ],
      "source": [
        "for class_idx in range(2): #We have only two classes\n",
        "  experiment(class_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiLmVhspYVN8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}